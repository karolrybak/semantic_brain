{
  "version": "0.0.167",
  "files": [
    {
      "filename": "server/ai/core.ts",
      "content": "import { Module, Type, type } from \"arktype\";\nimport { AI_STATE } from \"./state\";\nimport { Schemas, getJsonSchema } from \"./schemas\";\nimport type { ServerConfig } from \"../config\";\n\nexport interface AIRunOptions {\n  prompt: string;\n  schema: type.Any;\n  config: ServerConfig;\n  taskName: string;\n  maxTokens?: number;\n  temperature?: number;\n}\n\nexport async function executeAITask<T>(\n  options: AIRunOptions\n): Promise<T | null> {\n  const { session, isAiBusy, llama } = AI_STATE;\n  const { prompt, schema, config, taskName, maxTokens = 400, temperature = 0.5 } = options;\n\n  if (!session || isAiBusy || !llama) return null;\n\n  AI_STATE.isAiBusy = true;\n  const startTime = performance.now();\n  if (config.logPrompts) {\n      console.info(`[AI] <<< ${taskName} PROMPT (${prompt}s):\\n`);\n  }\n  try {\n    const grammar = await llama.createGrammarForJsonSchema(schema.toJsonSchema());\n\n    const response = await session.prompt(prompt, {\n      maxTokens,\n      temperature,\n      grammar\n    });\n\n    const duration = ((performance.now() - startTime) / 1000).toFixed(2);\n    if (config.logPrompts) {\n      console.log(`[AI] <<< ${taskName} RESPONSE (${duration}s):\\n${response}`);\n    }\n\n    // Parse response string back to object\n    const data = typeof response === \"string\" ? JSON.parse(response) : response;\n    const result = schema(data);\n\n    if (result.problems) {\n      console.error(`[AI] ${taskName} validation failed:`, result.problems.summary);\n      return null;\n    }\n\n    return result.data || result;\n  } catch (e) {\n    console.error(`[AI] ${taskName} execution failed:`, e);\n    return null;\n  } finally {\n    AI_STATE.isAiBusy = false;\n  }\n}"
    },
    {
      "filename": "server/ai/index.ts",
      "content": "export * from \"./types\";\nexport * from \"./state\";\nexport * from \"./methods\";"
    },
    {
      "filename": "server/ai/methods.ts",
      "content": "import { executeAITask } from \"./core\";\nimport { Schemas } from \"./schemas\";\nimport { \n  ASPECT_SUGGESTION_PROMPT as DESCRIBE_PROMPT, \n  NEW_CONNECTIONS_PROMPT as newConnectionsPrompt, \n  FIND_CONNECTIONS_PROMPT, \n  EVALUATE_ASPECTS_PROMPT \n} from \"./prompts\";\nimport type { ServerConfig } from \"../config\";\n\nexport interface NewConnectionsParams {\n  label: string;\n  forbiddenNodes: string[];\n  aspectList: string[];\n  existingNodes: string[];\n  creativity: number;\n  config: ServerConfig;\n}\n\nexport async function describeConcept(label: string, config: ServerConfig): Promise<string[]> {\n  const result = await executeAITask<string[]>({\n    prompt: DESCRIBE_PROMPT(label),\n    schema: Schemas.AspectResponse,\n    config,\n    taskName: \"SUGGEST_ASPECTS\",\n    maxTokens: 200\n  });\n  return result || [];\n}\n\nexport async function newConnections(params: NewConnectionsParams): Promise<typeof Schemas.ConnectionResponse.infer> {\n  const { label, forbiddenNodes, existingNodes, creativity, config } = params;\n  const forbiddenStr = forbiddenNodes.join(\", \");\n  const existingStr = existingNodes.join(\", \");\n\n  const prompt = newConnectionsPrompt(label, existingStr, forbiddenStr);\n\n  const result = await executeAITask<typeof Schemas.ConnectionResponse.infer>({\n    prompt,\n    schema: Schemas.ConnectionResponse,\n    config,\n    taskName: `NEW_CONNECTIONS`,\n    temperature: creativity,\n    maxTokens: 1000\n  });\n\n  return result || { connections: [] };\n}\n\nexport interface FindConnectionsParams {\n  label: string;\n  existingNodes: string[];\n  config: ServerConfig;\n}\n\nexport async function findExistingConnections(params: FindConnectionsParams): Promise<typeof Schemas.ConnectionResponse.infer> {\n  const { label, existingNodes, config } = params;\n  const existingStr = existingNodes.join(\", \");\n\n  const prompt = FIND_CONNECTIONS_PROMPT(label, existingStr);\n\n  const result = await executeAITask<typeof Schemas.ConnectionResponse.infer>({\n    prompt,\n    schema: Schemas.ConnectionResponse,\n    config,\n    taskName: `FIND_EXISTING_CONNECTIONS`,\n    temperature: 0.3,\n    maxTokens: 1000\n  });\n\n  return result || { connections: [] };\n}\n\n\n\n\nexport async function describeNode(\n  label: string, \n  aspectList: string[], \n  config: ServerConfig\n): Promise<{ description: string; aspects: Record<string, number> } | null> {\n  \n  const result = await executeAITask<typeof Schemas.Node.infer>({\n    prompt: EVALUATE_ASPECTS_PROMPT(label, aspectList.join(\", \")),\n    schema: Schemas.Node,\n    config,\n    taskName: \"EVALUATE_ASPECTS\",\n    temperature: 0.1,\n    maxTokens: 500\n  });\n\n  if (!result) return null;\n\n  const aspects: Record<string, number> = {};\n  if (Array.isArray(result.scores)) {\n    result.scores.forEach((s: any) => {\n      aspects[s.aspect] = s.rating;\n    });\n  }\n\n  return {\n    description: result.description,\n    aspects\n  };\n}"
    },
    {
      "filename": "server/ai/prompts.ts",
      "content": "/**\n * Global System Prompt\n */\nexport const SYSTEM_PROMPT = `\nYou are a knowledge graph generator.\nYour job is to expand a conceptual graph using specific nodes and relations.\n`;\n\n/**\n * Suggest Aspects Prompt\n */\nexport const ASPECT_SUGGESTION_PROMPT = (label: string) => `\n### INSTRUCTION\nSuggest 6-8 distinct semantic dimensions (aspects) for a deep multi-perspective analysis of the concept below.\nReturn as a simple JSON array of strings.\n\n### INPUT\nConcept: \"${label}\"\n`;\n\n/**\n * Brainstorm New Prompt\n */\nexport const NEW_CONNECTIONS_PROMPT = (label: string, existingStr: string, forbiddenStr: string) => `\n### INSTRUCTION\nGenerate 3-5 NEW unique concepts related to \"${label}\" \nProvide varied relation types and concepts.\n\n### INPUT\n- Target Concept: \"${label}\"\n- Existing Graph Labels: [${existingStr}]\n- Forbidden Labels: [${forbiddenStr}]\n`;\n\n/**\n * Brainstorm Existing Prompt\n */\nexport const FIND_CONNECTIONS_PROMPT = (label: string, existingStr: string) => `\n### INSTRUCTION\nFind logical links between \"${label}\" and the candidate concepts below.\n\n### INPUT\n- Target Concept: \"${label}\"\n- Candidate Concepts: [${existingStr}]\n`;\n\n/**\n * Evaluate Aspects Prompt\n */\nexport const EVALUATE_ASPECTS_PROMPT = (label: string, aspectStr: string) => `\n### INSTRUCTION\nProvide short 10-20 words description of the concept.\nAssign relevance scores (0.0 to 1.0) for each of specified aspects.\n\n### INPUT\n- Concept: \"${label}\"\n- Aspects: [${aspectStr}]\n`;\n\n"
    },
    {
      "filename": "server/ai/response.ts",
      "content": "import { Schemas } from \"./schemas\";\nimport type { Operation } from \"fast-json-patch\";\nimport type { GraphData, GraphLink, GraphNode, GraphState } from \"../../src/types/graph\";\n\nexport function shortRandomHash(length: 6 | 8 = 8): string {\n  const bytesLen = Math.ceil((length * 3) / 4);\n  const bytes = new Uint8Array(bytesLen);\n  crypto.getRandomValues(bytes);\n\n  return Buffer.from(bytes).toString(\"base64url\").slice(0, length);\n}\n/**\n * Tworzy patch (fast-json-patch) dodający nody i linki do GraphData.\n * Każdy input node zostanie połączony linkiem: source=rootId -> target=newNodeId.\n */\nexport function aiListToGraphPatch(\n  rootId: string,\n  state: GraphState,\n  input: typeof Schemas.ConnectionResponse.infer,\n): Operation[] {\n\n\n  const ops: Operation[] = [];\n\n  for (const n of input.connections  ?? []) {\n    const nodeId = shortRandomHash();\n\n    const graphNode: GraphNode = {\n      id: nodeId,\n      label: n.target,\n      status: 'accepted',\n      type: \"concept\",\n      val: 1,\n      aspects: {},\n    };\n\n    const linkId = shortRandomHash();\n\n    const graphLink: GraphLink = {\n      id: linkId,\n      source: rootId,\n      target: nodeId,\n      type: \"ai\",\n      relationType: n.relation\n    };\n\n    ops.push({ op: \"add\", path: `/nodes/${nodeId}`, value: graphNode });\n    ops.push({ op: \"add\", path: `/links/-`, value: graphLink });\n  }\n\n  return ops;\n}"
    },
    {
      "filename": "server/ai/schemas.ts",
      "content": "import { type } from \"arktype\";\nimport { Connect } from \"vite\";\n\nconst Relations = \"'enables' | 'causes' | 'conflicts_with' | 'depends_on' | 'example_of' | 'part_of' | 'risk_of' | 'opportunity_for' | 'similar_to' | 'opposite_of'\";\n\n/**\n * Unified Graph Schema Module\n */\nexport const Schemas = type.module({\n    Node: {\n        label: \"string\",\n        scores: \"AspectRating[]\",\n        description: \"string\"\n    },\n    Connection: {\n        target: \"string\",\n        relation: Relations,\n    },\n    ConnectionResponse: {\n        connections: \"Connection[]\",\n    },\n    AspectResponse: \"string[]\",\n    AspectRating: {\n        aspect: \"string\",\n        rating: \"number\"\n    }\n});\n\n/**\n * Helper to convert Arktype schemas to JSON Schema for the LLM grammar\n */\nexport function getJsonSchema(arkType: any) {\n    return arkType.toJsonSchema();\n}"
    },
    {
      "filename": "server/ai/state.ts",
      "content": "import { existsSync } from \"fs\";\nimport { getLlama, LlamaChatSession } from \"node-llama-cpp\";\nimport type { AIState } from \"./types\";\nimport type { ServerConfig } from \"../config\";\nimport { SYSTEM_PROMPT } from \"./prompts\";\n\nexport const AI_STATE: AIState = {\n  llama: null,\n  model: null,\n  context: null,\n  session: null,\n  isAiBusy: false,\n};\n\nexport async function unloadAI(): Promise<void> {\n  console.log(\"[AI] Unloading model...\");\n  AI_STATE.session = null;\n  AI_STATE.context = null;\n  AI_STATE.model = null;\n  AI_STATE.isAiBusy = false;\n}\n\nexport async function initializeAI(\n  config: ServerConfig,\n  onReady?: (error?: string) => void\n): Promise<void> {\n  const path = config.modelPaths[config.selectedSize];\n  \n  if (AI_STATE.model) {\n    await unloadAI();\n  }\n\n  if (!path || !existsSync(path)) {\n    const err = `Model file not found: ${path}`;\n    console.error(`[AI] ${err}`);\n    onReady?.(err);\n    return;\n  }\n\n  try {\n    console.log(`\\n--- [AI INITIALIZATION] size: ${config.selectedSize} ---`);\n    if (!AI_STATE.llama) {\n      AI_STATE.llama = await getLlama();\n    }\n    \n    AI_STATE.model = await AI_STATE.llama.loadModel({ modelPath: path });\n    if (!AI_STATE.model) throw new Error(\"Failed to load model\");\n    \n    AI_STATE.context = await AI_STATE.model.createContext();\n    AI_STATE.session = new LlamaChatSession({\n      contextSequence: AI_STATE.context!.getSequence(),\n      systemPrompt: SYSTEM_PROMPT\n    });\n\n    console.log(\"[AI] READY: Semantic Engine online.\");\n    onReady?.();\n  } catch (e: any) {\n    console.error(\"[AI] Load error\", e);\n    onReady?.(e.message || \"Unknown error\");\n  }\n}\n\nexport function getAIStatus(): 'unloaded' | 'ready' | 'loading' | 'error' {\n    if (AI_STATE.model && AI_STATE.session) return 'ready';\n    return 'unloaded';\n}"
    },
    {
      "filename": "server/ai/types.ts",
      "content": "import { LlamaModel, LlamaContext, LlamaChatSession } from \"node-llama-cpp\";\n\nexport interface AIState {\n  llama: any;\n  model: LlamaModel | null;\n  context: LlamaContext | null;\n  session: LlamaChatSession | null;\n  isAiBusy: boolean;\n}"
    },
    {
      "filename": "server/auto-explore.ts",
      "content": "import type { GraphState, GraphNode } from \"../src/types/graph\";\nimport { AI_STATE, newConnections, describeNode } from \"./ai/index\";\nimport { addAIGeneratedNodes, applyStatePatches } from \"./state\";\nimport type { Operation } from \"fast-json-patch\";\nimport type { ServerConfig } from \"./config\";\n\nexport interface AutoExploreContext {\n  state: GraphState;\n  config: ServerConfig;\n  broadcast: (payload: any) => void;\n  triggerSave: () => void;\n}\n\nlet autoExploreInterval: NodeJS.Timeout | null = null;\n\nexport function startAutoExplore(context: AutoExploreContext): NodeJS.Timeout {\n  if (autoExploreInterval) clearInterval(autoExploreInterval);\n\n  autoExploreInterval = setInterval(async () => {\n    await runAutoExploreIteration(context);\n  }, 5000);\n\n  return autoExploreInterval;\n}\n\nexport function stopAutoExplore(): void {\n  if (autoExploreInterval) {\n    clearInterval(autoExploreInterval);\n    autoExploreInterval = null;\n  }\n}\n\nasync function runAutoExploreIteration(context: AutoExploreContext): Promise<void> {\n  const { state, config, broadcast, triggerSave } = context;\n\n  // Exit if AI is busy or model not loaded\n  if (AI_STATE.isAiBusy || !AI_STATE.model) return;\n\n  const needsMetadata = (n: GraphNode) => n.status === \"accepted\" && (!n.description || !n.aspects || Object.keys(n.aspects).length === 0);\n  const needsConnections = (n: GraphNode) => {\n    if (n.status !== \"accepted\" && !state.settings.autoExplore) return false;\n    const links = state.links.filter(l => l.source === n.id || l.target === n.id);\n    return links.length < state.settings.minConnections;\n  };\n\n  const allNodes = Object.values(state.nodes);\n  let targetNode = null;\n\n  // PRIORITY 1: Metadata Enrichment (Clean up the graph first)\n  // Check focus first, then others\n  if (state.focusNodeId && state.nodes[state.focusNodeId] && needsMetadata(state.nodes[state.focusNodeId])) {\n    targetNode = state.nodes[state.focusNodeId];\n  } else {\n    targetNode = allNodes.find(needsMetadata);\n  }\n\n  // PRIORITY 2: New Connections (Only if all accepted nodes are described AND autoExplore is enabled)\n  if (!targetNode && state.settings.autoExplore) {\n    if (state.focusNodeId && state.nodes[state.focusNodeId] && needsConnections(state.nodes[state.focusNodeId])) {\n      targetNode = state.nodes[state.focusNodeId];\n    } else {\n      targetNode = allNodes.find(needsConnections);\n    }\n  }\n\n  if (!targetNode) return;\n\n  // Check if we should DESCRIBE or EXPLORE\n  const mode = needsMetadata(targetNode) ? 'DESCRIBE' : 'EXPLORE';\n\n  // Set thinking state\n  applyStatePatches(state, [\n    { op: \"replace\", path: \"/thinkingNodeId\", value: targetNode.id },\n  ]);\n  broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/thinkingNodeId\", value: targetNode.id }] });\n\n  if (mode === 'DESCRIBE') {\n    const nodeInfo = await describeNode(targetNode.label, state.settings.definedAspects, config);\n    \n    // Clear thinking state\n    const patches: Operation[] = [\n      { op: \"replace\", path: \"/thinkingNodeId\", value: null }\n    ];\n\n    if (nodeInfo) {\n      patches.push({ op: \"replace\", path: `/nodes/${targetNode.id}/description`, value: nodeInfo.description });\n      patches.push({ op: \"replace\", path: `/nodes/${targetNode.id}/aspects`, value: nodeInfo.aspects });\n    }\n\n    applyStatePatches(state, patches);\n    broadcast({ type: \"PATCH\", patches });\n    triggerSave();\n  } else {\n    // Generate connections\n    const neighborIds = state.links\n      .filter(l => l.source === targetNode.id || l.target === targetNode.id)\n      .map(l => l.source === targetNode.id ? l.target : l.source);\n    \n    const contextLabels = Array.from(new Set(neighborIds))\n      .map(id => state.nodes[id]?.label)\n      .filter(Boolean)\n      .slice(0, 10);\n\n    const forbiddenLabels = Object.values(state.nodes)\n      .filter(n => n.status === \"forbidden\")\n      .map(n => n.label.toLowerCase());\n\n    const suggestions = await newConnections({\n      label: targetNode.label,\n      forbiddenNodes: forbiddenLabels,\n      existingNodes: contextLabels,\n      creativity: state.settings.creativity,\n      config\n    });\n\n    // Clear thinking and apply changes\n    const patches: Operation[] = [\n      { op: \"replace\", path: \"/thinkingNodeId\", value: null },\n    ];\n\n    applyStatePatches(state, patches);\n    broadcast({ type: \"PATCH\", patches });\n\n    if (suggestions && suggestions.connections.length > 0) {\n      const ops = addAIGeneratedNodes(state, targetNode.id, suggestions);\n      applyStatePatches(state, ops);\n      broadcast({ type: \"PATCH\", patches: ops });\n      triggerSave();\n    }\n  }\n}\n"
    },
    {
      "filename": "server/config.json",
      "content": "{\n  \"modelPaths\": {\n    \"small\": \"/mnt/ssd/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf\",\n    \"medium\": \"/mnt/ssd/qwen2.5-7b-instruct-q4_k_m.gguf\",\n    \"large\": \"/mnt/ssd/qwen2.5-14b-instruct-q4_k_m.gguf\"\n  },\n  \"selectedSize\": \"large\",\n  \"loadOnStartup\": false,\n  \"logPrompts\": true\n}"
    },
    {
      "filename": "server/config.ts",
      "content": "export interface ServerConfig {\n  modelPaths: {\n    small: string;\n    medium: string;\n    large: string;\n  };\n  selectedSize: 'small' | 'medium' | 'large';\n  loadOnStartup: boolean;\n  logPrompts: boolean;\n}\n\nexport const DEFAULT_CONFIG: ServerConfig = {\n  modelPaths: {\n    small: \"\",\n    medium: \"\",\n    large: \"\",\n  },\n  selectedSize: 'medium',\n  loadOnStartup: true,\n  logPrompts: false,\n};\n\nexport async function loadConfig(configPath: string): Promise<ServerConfig> {\n  try {\n    const confFile = Bun.file(configPath);\n    if (await confFile.exists()) {\n      const config = await confFile.json();\n      console.log(\"[Config] Server config loaded.\");\n      return { ...DEFAULT_CONFIG, ...config };\n    }\n  } catch (e) {\n    console.error(\"[Config] Failed to load config.json\");\n  }\n  return DEFAULT_CONFIG;\n}\n"
    },
    {
      "filename": "server/config_large.json",
      "content": "{\n  \"modelPath\": \"/mnt/ssd/qwen2.5-14b-instruct-q4_k_m.gguf\",\n  \"logPrompts\": true\n}"
    },
    {
      "filename": "server/config_s.json",
      "content": "{\n  \"modelPath\": \"/mnt/ssd/Qwen2.5-0.5B-Instruct-Q5_K_M.gguf\",\n  \"logPrompts\": true\n}"
    },
    {
      "filename": "server/index.ts",
      "content": "import { serve } from \"bun\";\nimport type { GraphState } from \"../src/types/graph\";\nimport { join } from \"path\";\nimport { Schemas, getJsonSchema } from \"./ai/schemas\";\n\n// Handle CLI Flags\nconst args = process.argv.slice(2);\nif (args.includes(\"-d\") || args.includes(\"--dump-schema\")) {\n  const schemas = {\n    AspectList: getJsonSchema(Schemas.AspectResponse),\n    Association: getJsonSchema(Schemas.ConnectionResponse),\n    Eval: getJsonSchema(Schemas.Node)\n  };\n  console.log(JSON.stringify(schemas, null, 2));\n  process.exit(0);\n}\n\n// Module imports\nimport { loadConfig } from \"./config\";\nimport {\n  initializeDataDirectory,\n  DATA_DIR,\n  STATE_PATH,\n  loadStateFromDisk,\n  triggerDebouncedSave,\n} from \"./persistence\";\nimport {\n  createDefaultState,\n  initializeLoadedState,\n} from \"./state\";\nimport { initializeAI, AI_STATE, getAIStatus } from \"./ai/index\";\nimport { startAutoExplore } from \"./auto-explore\";\nimport {\n  broadcast,\n  createWSMessageHandler,\n} from \"./ws-handlers\";\n\nconsole.log(\"\\n--- [STARTING BRAIN SERVER S2] ---\");\n\nconst CONFIG_PATH = join(import.meta.dir, \"config.json\");\nconst clients = new Set<any>();\n\nlet state: GraphState;\nlet serverConfig = await loadConfig(CONFIG_PATH);\n\n// Initialize state\ninitializeDataDirectory();\n\nconst loadedState = await loadStateFromDisk(STATE_PATH);\nif (loadedState) {\n  state = initializeLoadedState(loadedState);\n  console.log(\"[State] Graph state loaded from disk.\");\n} else {\n  state = createDefaultState();\n  console.log(\"[State] Created new graph state.\");\n}\n\n// Initialize AI if startup flag is set\nif (serverConfig.loadOnStartup) {\n  initializeAI(serverConfig, () => {\n    broadcast(clients, { type: \"AI_STATUS\", status: \"ready\", size: serverConfig.selectedSize });\n  });\n}\n\n// Create handler context\nconst handlerContext = {\n  state,\n  config: serverConfig,\n  configPath: CONFIG_PATH,\n  statePath: STATE_PATH,\n  broadcast: (payload: any) => broadcast(clients, payload),\n};\n\n// Start the background auto-exploration agent\nstartAutoExplore({\n  state,\n  config: serverConfig,\n  broadcast: (payload: any) => broadcast(clients, payload),\n  triggerSave: () => triggerDebouncedSave(STATE_PATH, state),\n});\n\n// Start HTTP/WebSocket server\nserve({\n  port: 3001,\n  hostname: \"0.0.0.0\",\n  fetch(req, server) {\n    if (server.upgrade(req)) return;\n    return new Response(\"Brain Server S2 Online\");\n  },\n  websocket: {\n    open(ws) {\n      clients.add(ws);\n      // Initial sync for the client\n      ws.send(JSON.stringify({ type: \"FULL_STATE\", state }));\n      ws.send(JSON.stringify({ type: \"AI_CONFIG_UPDATED\", config: serverConfig }));\n      ws.send(JSON.stringify({ type: \"AI_STATUS\", status: getAIStatus() }));\n      console.log(`[WS] Client linked. Count: ${clients.size}`);\n    },\n    message: createWSMessageHandler(handlerContext),\n    close(ws) {\n      clients.delete(ws);\n    },\n  },\n});"
    },
    {
      "filename": "server/persistence.ts",
      "content": "import { join } from \"path\";\nimport { mkdirSync, existsSync } from \"fs\";\n\nexport const DATA_DIR = join(process.cwd(), \"data\");\nexport const STATE_PATH = join(DATA_DIR, \"state.json\");\n\nexport function initializeDataDirectory(): void {\n  if (!existsSync(DATA_DIR)) {\n    mkdirSync(DATA_DIR);\n  }\n}\n\nexport async function loadStateFromDisk(statePath: string): Promise<any> {\n  try {\n    const file = Bun.file(statePath);\n    if (await file.exists()) {\n      return await file.json();\n    }\n  } catch (e) {\n    console.error(\"[Persistence] Failed to load state.json\");\n  }\n  return null;\n}\n\nexport async function saveStateToDisk(statePath: string, state: any): Promise<void> {\n  await Bun.write(statePath, JSON.stringify(state, null, 2));\n}\n\nlet saveTimeout: NodeJS.Timeout | null = null;\n\nexport function triggerDebouncedSave(\n  statePath: string,\n  state: any,\n  delayMs: number = 1000\n): void {\n  if (saveTimeout) clearTimeout(saveTimeout);\n  saveTimeout = setTimeout(() => {\n    saveStateToDisk(statePath, state).catch(e => \n      console.error(\"[Persistence] Save error\", e)\n    );\n  }, delayMs);\n}\n\nexport function cancelPendingSave(): void {\n  if (saveTimeout) {\n    clearTimeout(saveTimeout);\n    saveTimeout = null;\n  }\n}\n"
    },
    {
      "filename": "server/state.ts",
      "content": "import { applyPatch, type Operation } from \"fast-json-patch\";\nimport type { GraphState } from \"../src/types/graph\";\nimport type { Schemas } from \"./ai/schemas\";\n\nexport function createDefaultState(): GraphState {\n  return {\n    nodes: {},\n    links: [],\n    focusNodeId: null,\n    thinkingNodeId: null,\n    settings: {\n      creativity: 0.7,\n      maxWords: 3,\n      minConnections: 3,\n      autoExplore: false,\n      definedAspects: [],\n      activeAspects: [],\n    },\n  };\n}\n\nexport function initializeLoadedState(state: GraphState): GraphState {\n  state.thinkingNodeId = null;\n  if (!state.settings.definedAspects) {\n    state.settings.definedAspects = [];\n  }\n  if (!state.settings.activeAspects) {\n    state.settings.activeAspects = [];\n  }\n  return state;\n}\n\nexport function applyStatePatches(\n  state: GraphState,\n  patches: Operation[]\n): void {\n  applyPatch(state, patches);\n}\n\nexport function addNodeToState(\n  state: GraphState,\n  label: string,\n  parentId?: string\n): { nodeId: string; ops: Operation[] } {\n  const id = Math.random().toString(36).slice(2, 9);\n  const isFirst = Object.keys(state.nodes).length === 0;\n  const newNode = {\n    id,\n    label,\n    status: \"accepted\" as const,\n    type: isFirst ? (\"root\" as const) : (\"concept\" as const),\n    val: isFirst ? 5 : 3,\n    aspects: {} as Record<string, number>,\n  };\n\n  const ops: Operation[] = [{ op: \"add\", path: `/nodes/${id}`, value: newNode }];\n  if (parentId) {\n    ops.push({\n      op: \"add\",\n      path: \"/links/-\",\n      value: { source: parentId, target: id, type: \"user\" },\n    });\n  }\n  if (isFirst) {\n    ops.push({ op: \"replace\", path: \"/focusNodeId\", value: id });\n  }\n\n  return { nodeId: id, ops };\n}\n\nexport function deleteNodeFromState(state: GraphState, nodeId: string): Operation[] {\n  const remainingLinks = state.links.filter(\n    l => l.source !== nodeId && l.target !== nodeId\n  );\n  return [\n    { op: \"remove\", path: `/nodes/${nodeId}` },\n    { op: \"replace\", path: \"/links\", value: remainingLinks },\n  ];\n}\n\nexport function clearStateGraph(state: GraphState): void {\n  state.nodes = {};\n  state.links = [];\n  state.focusNodeId = null;\n  state.thinkingNodeId = null;\n}\nexport function shortRandomHash(length: 6 | 8 = 8): string {\n  const bytesLen = Math.ceil((length * 3) / 4);\n  const bytes = new Uint8Array(bytesLen);\n  crypto.getRandomValues(bytes);\n\n  return Buffer.from(bytes).toString(\"base64url\").slice(0, length);\n}\nexport function addAIGeneratedNodes(\n  state: GraphState,\n  targetNodeId: string,\n  suggestions: typeof Schemas.ConnectionResponse.infer\n): Operation[] {\n  const ops: Operation[] = [];\n  const existingNodes = Object.values(state.nodes);\n\n  suggestions.connections.forEach((s) => {\n    // Normalize label for comparison\n    const normalizedLabel = s.target.trim().toLowerCase();\n    const existing = existingNodes.find(n => n.label.trim().toLowerCase() === normalizedLabel);\n\n    if (existing) {\n      // Check if link already exists\n      const linkExists = state.links.some(l => \n        (l.source === targetNodeId && l.target === existing.id) ||\n        (l.source === existing.id && l.target === targetNodeId)\n      );\n      \n      if (!linkExists) {\n        ops.push({\n          op: \"add\",\n          path: \"/links/-\",\n          value: {\n            source: targetNodeId,\n            target: existing.id,\n            type: \"bridge\",\n            relationType: s.relation,\n          },\n        });\n      }\n    } else {\n      // Add as new proposed node\n      const id = shortRandomHash();\n      ops.push({\n        op: \"add\",\n        path: `/nodes/${id}`,\n        value: {\n          id,\n          label: s.target,\n          status: \"accepted\",\n          type: \"concept\",\n          val: 2,\n          aspects: {},\n        },\n      });\n      ops.push({\n        op: \"add\",\n        path: \"/links/-\",\n        value: {\n          source: targetNodeId,\n          target: id,\n          type: \"ai\",\n          relationType: s.relation,\n        },\n      });\n    }\n  });\n  return ops;\n}\n"
    },
    {
      "filename": "server/tester.ts",
      "content": "import { join } from \"path\";\nimport { loadConfig } from \"./config\";\nimport { loadStateFromDisk, STATE_PATH } from \"./persistence\";\nimport { initializeAI, describeConcept, newConnections, describeNode } from \"./ai/index\";\nimport type { GraphState } from \"../src/types/graph\";\n\nconsole.log(\"\\n--- [BRAIN ENGINE AI TESTER] ---\");\n\nconst CONFIG_PATH = join(import.meta.dir, \"config.json\");\nconst config = await loadConfig(CONFIG_PATH);\nlet state: GraphState | null = await loadStateFromDisk(STATE_PATH);\n\nif (!state || !state.nodes || Object.keys(state.nodes).length === 0) {\n  console.error(\"Error: No state found or no nodes available in state.json. Please start the app and add a node first.\");\n  process.exit(1);\n}\n\nawait initializeAI(config, () => {\n  console.log(\"[AI] Model loaded and ready.\\n\");\n  startInteractiveShell();\n});\n\nasync function startInteractiveShell() {\n  let running = true;\n\n  while (running) {\n    // Reload state in case it changed externally\n    state = await loadStateFromDisk(STATE_PATH);\n    const nodes = Object.values(state!.nodes);\n    \n    console.log(\"\\n=== Available Nodes ===\");\n    nodes.forEach((n, i) => console.log(`  [${i}] ${n.label}`));\n    console.log(\"  [q] Quit\");\n\n    const nodeInput = prompt(\"\\nSelect node index or 'q' to quit:\") || \"\";\n    \n    if (nodeInput.toLowerCase() === 'q') {\n      running = false;\n      break;\n    }\n\n    const targetNode = nodes[parseInt(nodeInput)];\n    if (!targetNode) {\n      console.log(\"Invalid node selection.\");\n      continue;\n    }\n\n    let nodeLoop = true;\n    while (nodeLoop) {\n      console.log(`\\n--- Active Node: [${targetNode.label}] ---`);\n      console.log(\"  [1] Suggest Aspects\");\n      console.log(\"  [2] Brainstorm Related Concepts\");\n      console.log(\"  [3] Brainstorm Challenges\");\n      console.log(\"  [4] Evaluate Current Aspects\");\n      console.log(\"  [b] Back to Node Selection\");\n      console.log(\"  [q] Quit\");\n\n      const taskIdx = prompt(\"\\nSelect task (1-4, b, q):\") || \"\";\n\n      if (taskIdx.toLowerCase() === 'q') {\n        process.exit(0);\n      }\n      \n      if (taskIdx.toLowerCase() === 'b') {\n        nodeLoop = false;\n        continue;\n      }\n\n      console.log(`\\n--- Executing Task for: ${targetNode.label} ---\\n`);\n\n      try {\n        switch (taskIdx) {\n          case \"1\":\n            const aspects = await describeConcept(targetNode.label, config);\n            console.log(\"RESULT:\", JSON.stringify(aspects, null, 2));\n            break;\n          case \"2\":\n            const newNodes = await newConnections({\n              label: targetNode.label,\n              focus: \"variety\",\n              forbiddenNodes: [],\n              aspectList: state!.settings.definedAspects,\n              existingNodes: Object.values(state!.nodes).map(n => n.label),\n              mode: 'new',\n              creativity: state!.settings.creativity,\n              config\n            });\n            console.log(\"RESULT:\", JSON.stringify(newNodes, null, 2));\n            break;\n          case \"3\":\n            const links = await newConnections({\n              label: targetNode.label,\n              focus: \"challenges or obstacles\",\n              forbiddenNodes: [],\n              aspectList: state!.settings.definedAspects,\n              existingNodes: Object.values(state!.nodes).map(n => n.label),\n              mode: 'existing',\n              creativity: state!.settings.creativity,\n              config\n            });\n            console.log(\"RESULT:\", JSON.stringify(links, null, 2));\n            break;\n          case \"4\":\n            const scores = await describeNode(targetNode.label, state!.settings.definedAspects, config);\n            console.log(\"RESULT:\", JSON.stringify(scores, null, 2));\n            break;\n          default:\n            console.log(\"Unknown task.\");\n        }\n      } catch (e) {\n        console.error(\"Task failed:\", e);\n      }\n    }\n  }\n\n  console.log(\"\\n--- Test Session Closed ---\");\n  process.exit(0);\n}\n"
    },
    {
      "filename": "server/ws-handlers.ts",
      "content": "import type { GraphState } from \"../src/types/graph\";\nimport type { ServerConfig } from \"./config\";\nimport { newConnections, findExistingConnections, describeConcept, describeNode, AI_STATE, initializeAI, unloadAI } from \"./ai/index\";\nimport {\n  addNodeToState,\n  applyStatePatches,\n  deleteNodeFromState,\n  clearStateGraph,\n  addAIGeneratedNodes,\n} from \"./state\";\nimport { triggerDebouncedSave } from \"./persistence\";\nimport type { Operation } from \"fast-json-patch\";\n\nexport interface WSHandlerContext {\n  state: GraphState;\n  config: ServerConfig;\n  configPath: string;\n  statePath: string;\n  broadcast: (payload: any) => void;\n}\n\nexport function broadcast(clients: Set<any>, payload: any): void {\n  const msg = JSON.stringify(payload);\n  clients.forEach(c => {\n    try {\n      c.send(msg);\n    } catch (e) {\n      // Client may be disconnected\n    }\n  });\n}\n\nexport async function handleWSMessage(\n  message: any,\n  context: WSHandlerContext\n): Promise<void> {\n  const { state, config, configPath, statePath, broadcast } = context;\n  const data = message;\n  \n  switch (data.type) {\n    case \"SET_FOCUS\":\n      applyStatePatches(state, [{ op: \"replace\", path: \"/focusNodeId\", value: data.nodeId }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/focusNodeId\", value: data.nodeId }] });\n      triggerDebouncedSave(statePath, state);\n      break;\n\n    case \"UPDATE_SETTINGS\":\n      applyStatePatches(state, [{ op: \"replace\", path: \"/settings\", value: data.settings }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/settings\", value: data.settings }] });\n      triggerDebouncedSave(statePath, state);\n      break;\n\n    case \"ADD_NODE\": {\n      const { nodeId, ops } = addNodeToState(state, data.label, data.parentId);\n      applyStatePatches(state, ops);\n      broadcast({ type: \"PATCH\", patches: ops });\n      triggerDebouncedSave(statePath, state);\n      break;\n    }\n\n    case \"ACCEPT_NODE\":\n      applyStatePatches(state, [{ op: \"replace\", path: `/nodes/${data.nodeId}/status`, value: \"accepted\" }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: `/nodes/${data.nodeId}/status`, value: \"accepted\" }] });\n      triggerDebouncedSave(statePath, state);\n      break;\n\n    case \"FORBID_NODE\":\n      applyStatePatches(state, [{ op: \"replace\", path: `/nodes/${data.nodeId}/status`, value: \"forbidden\" }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: `/nodes/${data.nodeId}/status`, value: \"forbidden\" }] });\n      triggerDebouncedSave(statePath, state);\n      break;\n\n    case \"DELETE_NODE\": {\n      const ops = deleteNodeFromState(state, data.nodeId);\n      applyStatePatches(state, ops);\n      broadcast({ type: \"PATCH\", patches: ops });\n      triggerDebouncedSave(statePath, state);\n      break;\n    }\n\n    case \"CLEAR_GRAPH\":\n      clearStateGraph(state);\n      broadcast({ type: \"FULL_STATE\", state });\n      triggerDebouncedSave(statePath, state);\n      break;\n\n    case \"LOAD_AI_MODEL\":\n      if (AI_STATE.isAiBusy) return;\n      broadcast({ type: \"AI_STATUS\", status: \"loading\" });\n      await initializeAI(config, (err) => {\n        if (err) broadcast({ type: \"AI_STATUS\", status: \"error\", error: err });\n        else broadcast({ type: \"AI_STATUS\", status: \"ready\", size: config.selectedSize });\n      });\n      break;\n\n    case \"UNLOAD_AI_MODEL\":\n      if (AI_STATE.isAiBusy) return;\n      await unloadAI();\n      broadcast({ type: \"AI_STATUS\", status: \"unloaded\" });\n      break;\n\n    case \"UPDATE_AI_CONFIG\": {\n      Object.assign(config, data.config);\n      try {\n        await Bun.write(configPath, JSON.stringify(config, null, 2));\n        console.log(\"[Config] Server config updated.\");\n      } catch (e) {\n        console.error(\"[Config] Failed to write config.json\", e);\n      }\n      broadcast({ type: \"AI_CONFIG_UPDATED\", config });\n      break;\n    }\n\n    case \"EXPLORE_NEW\":\n    case \"EXPLORE_EXISTING\": {\n      if (AI_STATE.isAiBusy || !AI_STATE.session) return;\n      const targetNode = state.nodes[data.nodeId];\n      if (!targetNode) return;\n\n      applyStatePatches(state, [{ op: \"replace\", path: \"/thinkingNodeId\", value: targetNode.id }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/thinkingNodeId\", value: targetNode.id }] });\n\n      const neighborIds = state.links\n        .filter(l => l.source === targetNode.id || l.target === targetNode.id)\n        .map(l => l.source === targetNode.id ? l.target : l.source);\n\n      const forbiddenLabels = Object.values(state.nodes)\n        .filter(n => n.status === \"forbidden\")\n        .map(n => n.label.toLowerCase());\n\n      let suggestions;\n      const contextLabels = Array.from(new Set(neighborIds))\n        .map(id => state.nodes[id]?.label)\n        .filter(Boolean)\n        .slice(0, 10);\n\n      if (data.type === \"EXPLORE_NEW\") {\n        suggestions = await newConnections({\n          label: targetNode.label,\n          forbiddenNodes: forbiddenLabels,\n          existingNodes: contextLabels,\n          creativity: state.settings.creativity,\n          config\n        });\n      } else {\n        const candidates = Object.values(state.nodes)\n          .filter(n => n.id !== targetNode.id && !neighborIds.includes(n.id) && n.status !== 'forbidden')\n          .map(n => n.label)\n          .slice(0, 15);\n\n        suggestions = await findExistingConnections({\n          label: targetNode.label,\n          existingNodes: candidates,\n          config\n        });\n      }\n\n      applyStatePatches(state, [{ op: \"replace\", path: \"/thinkingNodeId\", value: null }]);\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/thinkingNodeId\", value: null }] });\n\n      if (suggestions && suggestions.connections.length > 0) {\n        const ops = addAIGeneratedNodes(state, targetNode.id, suggestions);\n        applyStatePatches(state, ops);\n        broadcast({ type: \"PATCH\", patches: ops });\n        triggerDebouncedSave(statePath, state);\n      }\n      break;\n    }\n\n    case \"UPDATE_NODE_ASPECTS\": {\n      if (AI_STATE.isAiBusy || !AI_STATE.session) return;\n      const targetNode = state.nodes[data.nodeId];\n      if (!targetNode) return;\n\n      broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/thinkingNodeId\", value: targetNode.id }] });\n\n      const nodeInfo = await describeNode(targetNode.label, state.settings.definedAspects, config);\n\n      if (nodeInfo) {\n        const patches: Operation[] = [\n          { op: \"replace\", path: \"/thinkingNodeId\", value: null },\n          { op: \"replace\", path: `/nodes/${targetNode.id}/description`, value: nodeInfo.description },\n          { op: \"replace\", path: `/nodes/${targetNode.id}/aspects`, value: nodeInfo.aspects }\n        ];\n        applyStatePatches(state, patches);\n        broadcast({ type: \"PATCH\", patches });\n        triggerDebouncedSave(statePath, state);\n      } else {\n        broadcast({ type: \"PATCH\", patches: [{ op: \"replace\", path: \"/thinkingNodeId\", value: null }] });\n      }\n      break;\n    }\n  }\n}\n\nexport function createWSMessageHandler(context: WSHandlerContext) {\n  return async (ws: any, message: any) => {\n    try {\n      const data = JSON.parse(message.toString());\n      await handleWSMessage(data, context);\n\n      if (data.type === \"SUGGEST_ASPECTS\") {\n        const suggested = await describeConcept(data.label, context.config);\n        ws.send(JSON.stringify({ type: \"ASPECT_SUGGESTIONS\", suggestions: suggested }));\n      }\n    } catch (e) {\n      console.error(\"[WS] Message error\", e);\n    }\n  };\n}"
    }
  ]
}