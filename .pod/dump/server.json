{
  "version": "0.0.202",
  "files": [
    {
      "filename": "server/ai/core.ts",
      "content": "import { Module, Type, type } from \"arktype\";\nimport { AI_STATE } from \"./state\";\nimport { Schemas, getJsonSchema } from \"./schemas\";\nimport type { ServerConfig } from \"../config\";\n\nexport interface AIRunOptions {\n  prompt: string;\n  schema: type.Any;\n  config: ServerConfig;\n  taskName: string;\n  maxTokens?: number;\n  temperature?: number;\n}\n\nexport async function executeAITask<T>(\n  options: AIRunOptions\n): Promise<T | null> {\n  const { session, isAiBusy, llama } = AI_STATE;\n  const { prompt, schema, config, taskName, maxTokens = 400, temperature = 0.5 } = options;\n\n  if (!session || isAiBusy || !llama) return null;\n\n  AI_STATE.isAiBusy = true;\n  const startTime = performance.now();\n  if (config.logPrompts) {\n      console.info(`[AI] <<< ${taskName} PROMPT (${prompt}s):\\n`);\n  }\n  try {\n    const grammar = await llama.createGrammarForJsonSchema(schema.toJsonSchema());\n\n    const response = await session.prompt(prompt, {\n      maxTokens,\n      temperature,\n      grammar\n    });\n\n    const duration = ((performance.now() - startTime) / 1000).toFixed(2);\n    if (config.logPrompts) {\n      console.log(`[AI] <<< ${taskName} RESPONSE (${duration}s):\\n${response}`);\n    }\n\n    // Parse response string back to object\n    const data = typeof response === \"string\" ? JSON.parse(response) : response;\n    const result = schema(data);\n\n    if (result.problems) {\n      console.error(`[AI] ${taskName} validation failed:`, result.problems.summary);\n      return null;\n    }\n\n    return result.data || result;\n  } catch (e) {\n    console.error(`[AI] ${taskName} execution failed:`, e);\n    return null;\n  } finally {\n    AI_STATE.isAiBusy = false;\n  }\n}"
    },
    {
      "filename": "server/ai/index.ts",
      "content": "export * from \"./types\";\nexport * from \"./state\";\nexport * from \"./methods\";"
    },
    {
      "filename": "server/ai/methods.ts",
      "content": "import { executeAITask } from \"./core\";\nimport { Schemas } from \"./schemas\";\nimport {\n  DESCRIBE_PROMPT,\n  NEW_CONNECTIONS_PROMPT,\n  FIND_CONNECTIONS_PROMPT,\n  NEW_RELATIONS_LIMITED_PROMPT\n} from \"./prompts\";\nimport type { ServerConfig } from \"../config\";\n\nexport interface NewConnectionsParams {\n  label: string;\n  forbiddenNodes: string[];\n  aspectList: string[];\n  existingNodes: string[];\n  creativity: number;\n  config: ServerConfig;\n}\n\nexport async function newConnections(params: NewConnectionsParams): Promise<typeof Schemas.ConnectionResponse.infer> {\n  const { label, existingNodes, creativity, config, aspectList } = params;\n  const existingStr = existingNodes.join(\", \");\n\n  const prompt = NEW_CONNECTIONS_PROMPT(label, existingStr, aspectList.join(\", \"));\n\n  const result = await executeAITask<typeof Schemas.ConnectionResponse.infer>({\n    prompt,\n    schema: Schemas.ConnectionResponse,\n    config,\n    taskName: `NEW_CONNECTIONS`,\n    temperature: creativity,\n    maxTokens: 1000\n  });\n\n  return result || { connections: [] };\n}\n\nexport async function newConnectionsLimited(params: NewConnectionsParams & { relations: string[] }): Promise<typeof Schemas.ConnectionResponse.infer> {\n  const { label, existingNodes, creativity, config, aspectList, relations } = params;\n  const existingStr = existingNodes.join(\", \");\n\n  const prompt = NEW_RELATIONS_LIMITED_PROMPT(label, existingStr, aspectList.join(\", \"), relations);\n\n  const result = await executeAITask<typeof Schemas.ConnectionResponse.infer>({\n    prompt,\n    schema: Schemas.ConnectionResponse,\n    config,\n    taskName: `NEW_CONNECTIONS_LIMITED`,\n    temperature: creativity,\n    maxTokens: 1000\n  });\n\n  return result || { connections: [] };\n}\n\nexport interface FindConnectionsParams {\n  label: string;\n  existingNodes: string[];\n  config: ServerConfig;\n}\n\nexport async function findExistingConnections(params: FindConnectionsParams): Promise<typeof Schemas.ConnectionResponse.infer> {\n  const { label, existingNodes, config } = params;\n  const existingStr = existingNodes.join(\", \");\n  \n  const prompt = FIND_CONNECTIONS_PROMPT(label, existingStr);\n\n  const result = await executeAITask<typeof Schemas.ConnectionResponse.infer>({\n    prompt,\n    schema: Schemas.ConnectionResponse,\n    config,\n    taskName: `FIND_EXISTING_CONNECTIONS`,\n    temperature: 0.3,\n    maxTokens: 1000\n  });\n  if(!result) return { connections: [] };\n  \n  result.connections = result.connections.filter((c) => existingNodes.includes(c.target));\n  \n  return result;\n}\n\nexport async function describeNode(\n  label: string,\n  aspectList: string[],\n  config: ServerConfig\n): Promise<{ description: string; aspects: Record<string, number>, emoji: string } | null> {\n\n  const result = await executeAITask<typeof Schemas.Node.infer>({\n    prompt: DESCRIBE_PROMPT(label, aspectList.join(\", \")),\n    schema: Schemas.Node,\n    config,\n    taskName: \"EVALUATE_ASPECTS\",\n    temperature: 0.1,\n    maxTokens: 500\n  });\n\n  if (!result) return null;\n\n  const aspects: Record<string, number> = {};\n  if (Array.isArray(result.scores)) {\n    result.scores.forEach((s: any) => {\n      aspects[s.aspect] = s.rating;\n    });\n  }\n\n  return {\n    description: result.description,\n    aspects,\n    emoji: extractEmojis(result.emoji)\n  };\n}\n\nfunction extractEmojis(input: string): string {\n  const emojiRegex = /\\p{Extended_Pictographic}/gu;\n  const matches = input.match(emojiRegex);\n  return matches ? matches.slice(0, 3).join(\"\") : \"◾\";\n}"
    },
    {
      "filename": "server/ai/prompts.ts",
      "content": "import { label } from \"three/tsl\";\nimport type { GraphNode } from \"../../src/types/graph\";\n\n/**\n * Global System Prompt\n */\nexport const SYSTEM_PROMPT = `\nYou are a ontologic knowledge graph generator.\nYour job is to expand a conceptual graph using specific nodes and relations.\n`;\n\n/**\n * Suggest Aspects Prompt\n */\nexport const ASPECT_SUGGESTION_PROMPT = (label: string) => `\n### INSTRUCTION\nSuggest 6-8 distinct semantic dimensions (aspects) for a deep multi-perspective analysis of the concept below.\nReturn as a simple JSON array of strings.\n\n### INPUT\nConcept: \"${label}\"\n`;\n\n\nconst NEW_CONNECTIONS = `\n### TASK\nGenerate 3–5 NEW ontologic concepts related to the target concept.\n\n### STRICT RULES (very important)\n\n1) Avoid generic or corporate wording.\n   Forbidden examples:\n   - \"increased efficiency\"\n   - \"broader access\"\n   - \"innovation\", \"growth\", \"opportunities\"\n   - any abstract business/HR phrasing\n\n2) Every concept must be TANGIBLE and OBSERVABLE.\n   A good concept is something you could:\n   - see\n   - measure\n   - record\n   - physically experience\n   - or point to as a real-world phenomenon\n\n3) Avoid overlap with existing graph labels.\n\n4) Prefer 1-3 words for concept.\n   Avoid long sentences or paragraphs.\n`\n\n/**\n * Brainstorm New Prompt\n */\nexport const NEW_CONNECTIONS_PROMPT = (label: string, existingStr: string, aspects: string) => `\n${NEW_CONNECTIONS}\n\n5) Provide variety of possible relation types\n\n### INPUT\nTarget Concept: \"${label}\"\nExisting Graph Labels: [${existingStr}]\n`;\n\n\nexport const NEW_RELATIONS_LIMITED_PROMPT = (label: string, existingStr: string, aspects: string, relations: string[]) => `\n${NEW_CONNECTIONS}\n\n5) Provide only relations of types: [${relations.join(\", \")}]\n\n### INPUT\nTarget Concept: \"${label}\"\nExisting Graph Labels: [${existingStr}]\n`\n\n\n/**\n * Brainstorm Existing Prompt\n */\nexport const FIND_CONNECTIONS_PROMPT = (label: string, existingStr: string) => `\n### INSTRUCTION\nFind logical links between \"${label}\" and the candidate concepts below.\nOnly include direct relationships.\nDO NOT create new concepts, only use existing ones.\n\n### INPUT\n- Candidate Concepts: [${existingStr}]\n`;\n\n/**\n * Evaluate Aspects Prompt\n */\nexport const DESCRIBE_PROMPT = (label: string, aspectStr: string) => `\n### INSTRUCTION\nProvide short 10-20 words description of the concept -> \"${label}\".\nPick up to three emoji symbols to represent the concept, use as little as possible.\nFor each of the aspects specified below - assign relevance score (0.0 to 1.0)\n\n### INPUT\n- Aspects: [${aspectStr}]\n`;\n"
    },
    {
      "filename": "server/ai/response.ts",
      "content": "import { Schemas } from \"./schemas\";\nimport type { Operation } from \"fast-json-patch\";\nimport type { GraphData, GraphLink, GraphNode, GraphState } from \"../../src/types/graph\";\n\nexport function shortRandomHash(length: 6 | 8 = 8): string {\n  const bytesLen = Math.ceil((length * 3) / 4);\n  const bytes = new Uint8Array(bytesLen);\n  crypto.getRandomValues(bytes);\n\n  return Buffer.from(bytes).toString(\"base64url\").slice(0, length);\n}\n/**\n * Tworzy patch (fast-json-patch) dodający nody i linki do GraphData.\n * Każdy input node zostanie połączony linkiem: source=rootId -> target=newNodeId.\n */\nexport function aiListToGraphPatch(\n  rootId: string,\n  state: GraphState,\n  input: typeof Schemas.ConnectionResponse.infer,\n): Operation[] {\n\n\n  const ops: Operation[] = [];\n\n  for (const n of input.connections  ?? []) {\n    const nodeId = shortRandomHash();\n\n    const graphNode: GraphNode = {\n      id: nodeId,\n      label: n.target,\n      status: 'accepted',\n      type: \"concept\",\n      val: 1,\n      aspects: {},\n    };\n\n    const linkId = shortRandomHash();\n\n    const graphLink: GraphLink = {\n      id: linkId,\n      source: rootId,\n      target: nodeId,\n      type: \"ai\",\n      relationType: n.relation\n    };\n\n    ops.push({ op: \"add\", path: `/nodes/${nodeId}`, value: graphNode });\n    ops.push({ op: \"add\", path: `/links/-`, value: graphLink });\n  }\n\n  return ops;\n}"
    },
    {
      "filename": "server/ai/schemas.ts",
      "content": "import { type } from \"arktype\";\nimport { Connect } from \"vite\";\n\n// const Relations = \"'enables' | 'causes' | 'conflicts_with' | 'depends_on' | 'example_of' | 'part_of' | 'risk_of' | 'opportunity_for' | 'similar_to' | 'opposite_of' | 'subclass_of' | 'instance_of' | 'incompatible_with'\";\n\nexport const Relations = type.module({\n    hierarchical: `'subclass_of'`,\n\n    structural: `\n        'part_of' |\n        'has_part'\n    `,\n\n    causal: `\n        'causes' |\n        'enables' |\n        'depends_on'\n    `,\n\n    logical: `\n        'incompatible_with' |\n        'similar_to' |\n        'opposite_of'\n    `,\n\n    functional: `\n        causal\n    `,\n\n    all: `\n        hierarchical |\n        structural |\n        causal |\n        logical\n    `\n})\n\nexport const Schemas = type.module({\n    Node: {\n        label: \"string\",\n        scores: \"AspectRating[]\",\n        description: \"string\",\n        emoji: \"string\",\n    },\n    Connection: {\n        target: \"string\",\n        relation: Relations.all\n    },\n    ConnectionResponse: {\n        connections: \"Connection[]\",\n    },\n    AspectResponse: \"string[]\",\n    AspectRating: {\n        aspect: \"string\",\n        rating: \"number\"\n    },\n    SvgResponse: \"string\"\n});\n\n/**\n * Helper to convert Arktype schemas to JSON Schema for the LLM grammar\n */\nexport function getJsonSchema(arkType: any) {\n    return arkType.toJsonSchema();\n}"
    },
    {
      "filename": "server/ai/state.ts",
      "content": "import { existsSync } from \"fs\";\nimport { getLlama, LlamaChatSession } from \"node-llama-cpp\";\nimport type { AIState } from \"./types\";\nimport type { ServerConfig } from \"../config\";\nimport { SYSTEM_PROMPT } from \"./prompts\";\n\nexport const AI_STATE: AIState = {\n  llama: null,\n  model: null,\n  context: null,\n  session: null,\n  isAiBusy: false,\n  userQueue: [],\n};\n\nexport async function unloadAI(): Promise<void> {\n  console.log(\"[AI] Unloading model...\");\n  AI_STATE.session = null;\n  AI_STATE.context = null;\n  AI_STATE.model = null;\n  AI_STATE.isAiBusy = false;\n}\n\nexport async function initializeAI(\n  config: ServerConfig,\n  onReady?: (error?: string) => void\n): Promise<void> {\n  const path = config.modelPaths[config.selectedSize];\n  \n  if (AI_STATE.model) {\n    await unloadAI();\n  }\n\n  if (!path || !existsSync(path)) {\n    const err = `Model file not found: ${path}`;\n    console.error(`[AI] ${err}`);\n    onReady?.(err);\n    return;\n  }\n\n  try {\n    console.log(`\\n--- [AI INITIALIZATION] size: ${config.selectedSize} ---`);\n    if (!AI_STATE.llama) {\n      AI_STATE.llama = await getLlama();\n    }\n    \n    AI_STATE.model = await AI_STATE.llama.loadModel({ modelPath: path });\n    if (!AI_STATE.model) throw new Error(\"Failed to load model\");\n    \n    AI_STATE.context = await AI_STATE.model.createContext();\n    AI_STATE.session = new LlamaChatSession({\n      contextSequence: AI_STATE.context!.getSequence(),\n      systemPrompt: SYSTEM_PROMPT\n    });\n\n    console.log(\"[AI] READY: Semantic Engine online.\");\n    onReady?.();\n  } catch (e: any) {\n    console.error(\"[AI] Load error\", e);\n    onReady?.(e.message || \"Unknown error\");\n  }\n}\n\nexport function getAIStatus(): 'unloaded' | 'ready' | 'loading' | 'error' {\n    if (AI_STATE.model && AI_STATE.session) return 'ready';\n    return 'unloaded';\n}"
    },
    {
      "filename": "server/ai/types.ts",
      "content": "import { LlamaModel, LlamaContext, LlamaChatSession } from \"node-llama-cpp\";\n\nexport interface AITask {\n  type: 'DESCRIBE' | 'EXPLORE_NEW' | 'EXPLORE_EXISTING' | 'EXPLORE_LIMITED';\n  nodeId: string;\n  params?: any;\n}\n\nexport interface AIState {\n  llama: any;\n  model: LlamaModel | null;\n  context: LlamaContext | null;\n  session: LlamaChatSession | null;\n  isAiBusy: boolean;\n  userQueue: AITask[];\n}"
    },
    {
      "filename": "server/auto-explore.ts",
      "content": "import type { GraphState, GraphNode } from \"../src/types/graph\";\nimport { AI_STATE, newConnections, describeNode, findExistingConnections, newConnectionsLimited } from \"./ai/index\";\nimport { addAIGeneratedNodes } from \"./state\";\nimport type { ServerConfig } from \"./config\";\nimport type { AITask } from \"./ai/types\";\n\nexport interface AutoExploreContext {\n  state: GraphState;\n  config: ServerConfig;\n  broadcast: (payload: any) => void;\n  triggerSave: () => void;\n  sync: () => void;\n}\n\nlet autoExploreInterval: NodeJS.Timeout | null = null;\n\nexport function startAutoExplore(context: AutoExploreContext): NodeJS.Timeout {\n  if (autoExploreInterval) clearInterval(autoExploreInterval);\n\n  autoExploreInterval = setInterval(async () => {\n    await runAutoExploreIteration(context);\n  }, 100);\n\n  return autoExploreInterval;\n}\n\nexport function stopAutoExplore(): void {\n  if (autoExploreInterval) {\n    clearInterval(autoExploreInterval);\n    autoExploreInterval = null;\n  }\n}\n\nasync function runAutoExploreIteration(context: AutoExploreContext): Promise<void> {\n  const { state, config, sync } = context;\n\n  if (AI_STATE.isAiBusy || !AI_STATE.model) return;\n\n  const allNodes = Object.values(state.nodes);\n  if (allNodes.length === 0) return;\n\n  let task: AITask | null = null;\n\n  // 1. PRIORITY: User Queue\n  if (AI_STATE.userQueue.length > 0) {\n    task = AI_STATE.userQueue.shift()!;\n  }\n\n  // 2. AUTO-EXPLORE TASKS\n  if (!task) {\n    const getLinkCount = (id: string) => state.links.filter(l => l.source === id || l.target === id).length;\n\n    // Priority A: Metadata Enrichment (DESCRIBE)\n    const needsMetadata = allNodes.filter(n => n.status === \"accepted\" && (!n.emoji || !n.description || !n.aspects || Object.keys(n.aspects).length === 0));\n    if (needsMetadata.length > 0) {\n      const target = needsMetadata.sort((a, b) => (a.attempts?.DESCRIBE || 0) - (b.attempts?.DESCRIBE || 0))[0];\n      if(target)\n        task = { type: 'DESCRIBE', nodeId: target.id };\n    }\n\n    // Priority B: Find Existing Connections (EXPLORE_EXISTING)\n    if (!task && allNodes.length > 1 && state.links.length < allNodes.length * 1.5) {\n      const targets = allNodes.filter(n => n.status === 'accepted');\n      if (targets.length > 0) {\n        const target = targets.sort((a, b) => {\n          const linksA = getLinkCount(a.id);\n          const linksB = getLinkCount(b.id);\n          if (linksA !== linksB) return linksA - linksB;\n          return (a.attempts?.EXPLORE_EXISTING || 0) - (b.attempts?.EXPLORE_EXISTING || 0);\n        })[0];\n        if(target)\n          task = { type: 'EXPLORE_EXISTING', nodeId: target.id };\n      }\n    }\n\n    // Priority C: New Connections (EXPLORE_NEW)\n    if (!task && state.settings.autoExplore) {\n      const targets = allNodes.filter(n => n.status === 'accepted' && getLinkCount(n.id) < state.settings.minConnections);\n      if (targets.length > 0) {\n        const target = targets.sort((a, b) => (a.attempts?.EXPLORE_NEW || 0) - (b.attempts?.EXPLORE_NEW || 0))[0];\n        if(target)\n          task = { type: 'EXPLORE_NEW', nodeId: target.id };\n      }\n    }\n  }\n\n  if (!task) return;\n\n  const targetNode = state.nodes[task.nodeId];\n  if (!targetNode) return;\n\n  state.thinkingNodeId = targetNode.id;\n  sync();\n\n  try {\n    let success = false;\n\n    if (task.type === 'DESCRIBE') {\n      const nodeInfo = await describeNode(targetNode.label, state.settings.definedAspects, config);\n      if (nodeInfo) {\n        targetNode.description = nodeInfo.description;\n        targetNode.aspects = nodeInfo.aspects;\n        targetNode.emoji = nodeInfo.emoji;\n        success = true;\n      }\n    } else if (task.type === 'EXPLORE_NEW' || task.type === 'EXPLORE_LIMITED') {\n      const neighborIds = state.links\n        .filter(l => l.source === targetNode.id || l.target === targetNode.id)\n        .map(l => l.source === targetNode.id ? l.target : l.source);\n      \n      const contextLabels = Array.from(new Set(neighborIds))\n        .map(id => state.nodes[String(id)]?.label)\n        .filter((label): label is string => Boolean(label))\n        .slice(0, 10);\n\n      const forbiddenLabels = Object.values(state.nodes)\n        .filter(n => n.status === \"forbidden\")\n        .map(n => n.label.toLowerCase());\n\n      const suggestions = task.type === 'EXPLORE_NEW'\n        ? await newConnections({\n            label: targetNode.label,\n            forbiddenNodes: forbiddenLabels,\n            existingNodes: contextLabels,\n            creativity: state.settings.creativity,\n            aspectList: state.settings.definedAspects,\n            config\n          })\n        : await newConnectionsLimited({\n            label: targetNode.label,\n            forbiddenNodes: forbiddenLabels,\n            existingNodes: contextLabels,\n            creativity: state.settings.creativity,\n            aspectList: state.settings.definedAspects,\n            config,\n            relations: task.params.relations\n          });\n\n      if (suggestions && suggestions.connections.length > 0) {\n        addAIGeneratedNodes(state, targetNode.id, suggestions);\n        success = true;\n      }\n    } else if (task.type === 'EXPLORE_EXISTING') {\n      const currentNeighbors = state.links\n        .filter(l => l.source === targetNode.id || l.target === targetNode.id)\n        .map(l => l.source === targetNode.id ? l.target : l.source);\n      \n      const candidates = Object.values(state.nodes)\n        .filter(n => n.id !== targetNode.id && !currentNeighbors.includes(n.id) && n.status !== 'forbidden')\n        .map(n => n.label);\n\n      const candidateLabels = (task.params?.candidates || candidates.sort(() => Math.random() - 0.5)).slice(0, 10);\n\n      const suggestions = await findExistingConnections({\n        label: targetNode.label,\n        existingNodes: candidateLabels,\n        config\n      });\n\n      if (suggestions && suggestions.connections.length > 0) {\n        addAIGeneratedNodes(state, targetNode.id, suggestions);\n        success = true;\n      }\n    }\n\n    if (!success) {\n      if (!targetNode.attempts) targetNode.attempts = {};\n      targetNode.attempts[task.type] = (targetNode.attempts[task.type] || 0) + 1;\n    }\n\n  } finally {\n    state.thinkingNodeId = null;\n    sync();\n  }\n}"
    },
    {
      "filename": "server/config.ts",
      "content": "export interface ServerConfig {\n  modelPaths: {\n    small: string;\n    medium: string;\n    large: string;\n  };\n  selectedSize: 'small' | 'medium' | 'large';\n  loadOnStartup: boolean;\n  logPrompts: boolean;\n}\n\nexport const DEFAULT_CONFIG: ServerConfig = {\n  modelPaths: {\n    small: \"\",\n    medium: \"\",\n    large: \"\",\n  },\n  selectedSize: 'medium',\n  loadOnStartup: true,\n  logPrompts: false,\n};\n\nexport async function loadConfig(configPath: string): Promise<ServerConfig> {\n  try {\n    console.log(\"Loading config from\", configPath);\n    const confFile = Bun.file(configPath);\n    if (await confFile.exists()) {\n      const config = await confFile.json();\n      console.log(\"[Config] Server config loaded.\");\n      return { ...DEFAULT_CONFIG, ...config };\n    }\n  } catch (e) {\n    console.error(\"[Config] Failed to load config.json\");\n  }\n  return DEFAULT_CONFIG;\n}\n"
    },
    {
      "filename": "server/index.ts",
      "content": "import { serve } from \"bun\";\nimport type { GraphState } from \"../src/types/graph\";\nimport { join } from \"path\";\nimport { observe, generate } from \"fast-json-patch\";\nimport { Schemas, getJsonSchema } from \"./ai/schemas\";\n\n// Handle CLI Flags\nconst args = process.argv.slice(2);\nif (args.includes(\"-d\") || args.includes(\"--dump-schema\")) {\n  const schemas = {\n    AspectList: getJsonSchema(Schemas.AspectResponse),\n    Association: getJsonSchema(Schemas.ConnectionResponse),\n    Eval: getJsonSchema(Schemas.Node)\n  };\n  console.log(JSON.stringify(schemas, null, 2));\n  process.exit(0);\n}\n\n// Module imports\nimport { loadConfig } from \"./config\";\nimport {\n  initializeDataDirectory,\n  DATA_DIR,\n  STATE_PATH,\n  loadStateFromDisk,\n  triggerDebouncedSave,\n} from \"./persistence\";\nimport {\n  createDefaultState,\n  initializeLoadedState,\n} from \"./state\";\nimport { initializeAI, AI_STATE, getAIStatus } from \"./ai/index\";\nimport { startAutoExplore } from \"./auto-explore\";\nimport {\n  broadcast,\n  createWSMessageHandler,\n} from \"./ws-handlers\";\n\nconsole.log(\"\\n--- [STARTING BRAIN SERVER S2] ---\");\n\nconst CONFIG_PATH = join(import.meta.dir, \"./..\", \"config.json\");\nconst clients = new Set<any>();\n\nlet state: GraphState;\nlet serverConfig = await loadConfig(CONFIG_PATH);\n\n// Initialize state\ninitializeDataDirectory();\n\nconst loadedState = await loadStateFromDisk(STATE_PATH);\nif (loadedState) {\n  state = initializeLoadedState(loadedState);\n  console.log(\"[State] Graph state loaded from disk.\");\n} else {\n  state = createDefaultState();\n  console.log(\"[State] Created new graph state.\");\n}\n\nconst stateObserver = observe<GraphState>(state);\n\nconst sync = () => {\n  const patches = generate(stateObserver);\n  if (patches.length > 0) {\n    broadcast(clients, { type: \"PATCH\", patches });\n    triggerDebouncedSave(STATE_PATH, state);\n  }\n};\n\n// Initialize AI if startup flag is set\nif (serverConfig.loadOnStartup) {\n  initializeAI(serverConfig, () => {\n    broadcast(clients, { type: \"AI_STATUS\", status: \"ready\", size: serverConfig.selectedSize });\n  });\n}\n\n// Create handler context\nconst handlerContext = {\n  state,\n  config: serverConfig,\n  configPath: CONFIG_PATH,\n  statePath: STATE_PATH,\n  broadcast: (payload: any) => broadcast(clients, payload),\n  sync,\n};\n\n// Start the background auto-exploration agent\nstartAutoExplore({\n  state,\n  config: serverConfig,\n  broadcast: (payload: any) => broadcast(clients, payload),\n  triggerSave: () => triggerDebouncedSave(STATE_PATH, state),\n  sync,\n});\n\n// Start HTTP/WebSocket server\nserve({\n  port: 3001,\n  hostname: \"0.0.0.0\",\n  fetch(req, server) {\n    if (server.upgrade(req)) return;\n    return new Response(\"Brain Server S2 Online\");\n  },\n  websocket: {\n    open(ws) {\n      clients.add(ws);\n      // Initial sync for the client\n      ws.send(JSON.stringify({ type: \"FULL_STATE\", state }));\n      ws.send(JSON.stringify({ type: \"AI_CONFIG_UPDATED\", config: serverConfig }));\n      ws.send(JSON.stringify({ type: \"AI_STATUS\", status: getAIStatus() }));\n      console.log(`[WS] Client linked. Count: ${clients.size}`);\n    },\n    message: createWSMessageHandler(handlerContext),\n    close(ws) {\n      clients.delete(ws);\n    },\n  },\n});"
    },
    {
      "filename": "server/persistence.ts",
      "content": "import { join } from \"path\";\nimport { mkdirSync, existsSync } from \"fs\";\n\nexport const DATA_DIR = join(process.cwd(), \"data\");\nexport const STATE_PATH = join(DATA_DIR, \"state.json\");\n\nexport function initializeDataDirectory(): void {\n  if (!existsSync(DATA_DIR)) {\n    mkdirSync(DATA_DIR);\n  }\n}\n\nexport async function loadStateFromDisk(statePath: string): Promise<any> {\n  try {\n    const file = Bun.file(statePath);\n    if (await file.exists()) {\n      return await file.json();\n    }\n  } catch (e) {\n    console.error(\"[Persistence] Failed to load state.json\");\n  }\n  return null;\n}\n\nexport async function saveStateToDisk(statePath: string, state: any): Promise<void> {\n  await Bun.write(statePath, JSON.stringify(state, null, 2));\n}\n\nlet saveTimeout: NodeJS.Timeout | null = null;\n\nexport function triggerDebouncedSave(\n  statePath: string,\n  state: any,\n  delayMs: number = 1000\n): void {\n  if (saveTimeout) clearTimeout(saveTimeout);\n  saveTimeout = setTimeout(() => {\n    saveStateToDisk(statePath, state).catch(e => \n      console.error(\"[Persistence] Save error\", e)\n    );\n  }, delayMs);\n}\n\nexport function cancelPendingSave(): void {\n  if (saveTimeout) {\n    clearTimeout(saveTimeout);\n    saveTimeout = null;\n  }\n}\n"
    },
    {
      "filename": "server/state.ts",
      "content": "import { applyPatch, type Operation } from \"fast-json-patch\";\nimport type { GraphState } from \"../src/types/graph\";\nimport type { Schemas } from \"./ai/schemas\";\n\nexport function createDefaultState(): GraphState {\n  return {\n    nodes: {},\n    links: [],\n    focusNodeId: null,\n    thinkingNodeId: null,\n    settings: {\n      creativity: 0.7,\n      maxWords: 3,\n      minConnections: 3,\n      autoExplore: false,\n      definedAspects: [],\n      activeAspects: [],\n    },\n  };\n}\n\nexport function initializeLoadedState(state: GraphState): GraphState {\n  state.thinkingNodeId = null;\n  if (!state.settings.definedAspects) {\n    state.settings.definedAspects = [];\n  }\n  if (!state.settings.activeAspects) {\n    state.settings.activeAspects = [];\n  }\n  Object.values(state.nodes).forEach(n => {\n    if (!n.attempts) n.attempts = {};\n  });\n  return state;\n}\n\nexport function applyStatePatches(\n  state: GraphState,\n  patches: Operation[]\n): void {\n  applyPatch(state, patches);\n}\n\nexport function addNodeToState(\n  state: GraphState,\n  label: string,\n  parentId?: string\n): string {\n  const id = shortRandomHash();\n  const isFirst = Object.keys(state.nodes).length === 0;\n  \n  state.nodes[id] = {\n    id,\n    label,\n    status: \"accepted\",\n    type: isFirst ? \"root\" : \"concept\",\n    val: isFirst ? 5 : 3,\n    aspects: {},\n    attempts: {},\n  };\n\n  if (parentId) {\n    state.links.push({\n      source: parentId,\n      target: id,\n      type: \"user\"\n    });\n  }\n  \n  if (isFirst) {\n    state.focusNodeId = id;\n  }\n\n  return id;\n}\n\nexport function deleteNodeFromState(state: GraphState, nodeId: string): void {\n  delete state.nodes[nodeId];\n  state.links = state.links.filter(\n    l => l.source !== nodeId && l.target !== nodeId\n  );\n  if (state.focusNodeId === nodeId) state.focusNodeId = null;\n  if (state.thinkingNodeId === nodeId) state.thinkingNodeId = null;\n}\n\nexport function clearStateGraph(state: GraphState): void {\n  state.nodes = {};\n  state.links = [];\n  state.focusNodeId = null;\n  state.thinkingNodeId = null;\n}\nexport function shortRandomHash(length: 6 | 8 = 8): string {\n  const bytesLen = Math.ceil((length * 3) / 4);\n  const bytes = new Uint8Array(bytesLen);\n  crypto.getRandomValues(bytes);\n\n  return Buffer.from(bytes).toString(\"base64url\").slice(0, length);\n}\nexport function addAIGeneratedNodes(\n  state: GraphState,\n  targetNodeId: string,\n  suggestions: typeof Schemas.ConnectionResponse.infer\n): void {\n  const existingNodes = Object.values(state.nodes);\n\n  suggestions.connections.forEach((s) => {\n    const normalizedLabel = s.target.trim().toLowerCase();\n    const existing = existingNodes.find(n => n.label.trim().toLowerCase() === normalizedLabel);\n\n    if (existing) {\n      const linkExists = state.links.some(l => \n        (l.source === targetNodeId && l.target === existing.id) ||\n        (l.source === existing.id && l.target === targetNodeId)\n      );\n      \n      if (!linkExists) {\n        state.links.push({\n          source: targetNodeId,\n          target: existing.id,\n          type: \"bridge\",\n          relationType: s.relation,\n        });\n      }\n    } else {\n      const id = shortRandomHash();\n      state.nodes[id] = {\n        id,\n        label: s.target,\n        status: \"accepted\",\n        type: \"concept\",\n        val: 2,\n        aspects: {},\n      };\n      state.links.push({\n        source: targetNodeId,\n        target: id,\n        type: \"ai\",\n        relationType: s.relation,\n      });\n    }\n  });\n}\n"
    },
    {
      "filename": "server/ws-handlers.ts",
      "content": "import type { GraphState } from \"../src/types/graph\";\nimport type { ServerConfig } from \"./config\";\nimport { AI_STATE, initializeAI, unloadAI } from \"./ai/index\";\nimport {\n  addNodeToState,\n  deleteNodeFromState,\n  clearStateGraph,\n} from \"./state\";\n\nexport interface WSHandlerContext {\n  state: GraphState;\n  config: ServerConfig;\n  configPath: string;\n  statePath: string;\n  broadcast: (payload: any) => void;\n  sync: () => void;\n}\n\nexport function broadcast(clients: Set<any>, payload: any): void {\n  const msg = JSON.stringify(payload);\n  clients.forEach(c => {\n    try {\n      c.send(msg);\n    } catch (e) {\n      // Client may be disconnected\n    }\n  });\n}\n\nexport async function handleWSMessage(\n  message: any,\n  context: WSHandlerContext\n): Promise<void> {\n  const { state, config, configPath, broadcast, sync } = context;\n  const data = message;\n  \n  switch (data.type) {\n    case \"SET_FOCUS\":\n      state.focusNodeId = data.nodeId;\n      sync();\n      break;\n\n    case \"UPDATE_SETTINGS\":\n      Object.assign(state.settings, data.settings);\n      sync();\n      break;\n\n    case \"ADD_NODE\":\n      addNodeToState(state, data.label, data.parentId);\n      sync();\n      break;\n\n    case \"ACCEPT_NODE\": {\n      const tNode = state.nodes[data.nodeId];\n      if (tNode) {\n        tNode.status = \"accepted\";\n        sync();\n      }\n      break;\n    }\n\n    case \"FORBID_NODE\": {\n      const tNode = state.nodes[data.nodeId];\n      if (tNode) {\n        tNode.status = \"forbidden\";\n        sync();\n      }\n      break;\n    }\n\n    case \"DELETE_NODE\":\n      deleteNodeFromState(state, data.nodeId);\n      sync();\n      break;\n\n    case \"CLEAR_GRAPH\":\n      clearStateGraph(state);\n      broadcast({ type: \"FULL_STATE\", state });\n      sync();\n      break;\n\n    case \"LOAD_AI_MODEL\":\n      if (AI_STATE.isAiBusy) return;\n      broadcast({ type: \"AI_STATUS\", status: \"loading\" });\n      await initializeAI(config, (err) => {\n        if (err) broadcast({ type: \"AI_STATUS\", status: \"error\", error: err });\n        else broadcast({ type: \"AI_STATUS\", status: \"ready\", size: config.selectedSize });\n      });\n      break;\n\n    case \"UNLOAD_AI_MODEL\":\n      if (AI_STATE.isAiBusy) return;\n      await unloadAI();\n      broadcast({ type: \"AI_STATUS\", status: \"unloaded\" });\n      break;\n\n    case \"UPDATE_AI_CONFIG\":\n      Object.assign(config, data.config);\n      try {\n        await Bun.write(configPath, JSON.stringify(config, null, 2));\n        console.log(\"[Config] Server config updated.\");\n      } catch (e) {\n        console.error(\"[Config] Failed to write config.json\", e);\n      }\n      broadcast({ type: \"AI_CONFIG_UPDATED\", config });\n      break;\n\n    case \"EXPLORE_NEW\":\n      AI_STATE.userQueue.push({ type: 'EXPLORE_NEW', nodeId: data.nodeId });\n      break;\n\n    case \"EXPLORE_LIMITED\":\n      AI_STATE.userQueue.push({ type: 'EXPLORE_LIMITED', nodeId: data.nodeId, params: { relations: data.relations } });\n      break;\n\n    case \"EXPLORE_EXISTING\":\n      AI_STATE.userQueue.push({ type: 'EXPLORE_EXISTING', nodeId: data.nodeId });\n      break;\n\n    case \"UPDATE_NODE_ASPECTS\":\n      AI_STATE.userQueue.push({ type: 'DESCRIBE', nodeId: data.nodeId });\n      break;\n  }\n}\n\nexport function createWSMessageHandler(context: WSHandlerContext) {\n  return async (ws: any, message: any) => {\n    try {\n      const data = JSON.parse(message.toString());\n      await handleWSMessage(data, context);\n    } catch (e) {\n      console.error(\"[WS] Message error\", e);\n    }\n  };\n}"
    }
  ]
}